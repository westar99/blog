---

# **title: 'REVIEW-그래프와 머신러닝'**

# **date: '2022-07-06 09:00'**

---

## 데이터 분석(머신러닝, 딥러닝) 프로세스
- 데이터 불러오기
  + CSV, 오라클, MySQL, PostgreSQL, 클라우드 DB연동
- 탐색적 자료 분석
  + 데이터 전처리 및 가공
- 잠정적인 컬럼의 갯수를 지정
- 머신러닝 모델(=통계 모델링, t.test, 분산분석, 교차분석)
- 머신러닝 모델의 경우 배포(지금은 다루지 않음)
 + JSP-스프링 웹개발 시 배우게 됨.
- 통계 모델링 경우 p-value값 기준으로 귀무가설 및 대립가설 검정

- (공통) 결과보고서를 작성 필요.
 + .PPT준비

## 그래프 복습
- 수치형 데이터 시각화
- 범주형 데이터 시각화
- 데이터 관계 시각화
 + matplotlib 라이브러리 방법(복잡)
 + seaborn 라이브러리 방법(단순)

### 수치형 데이터 시각화


```python
import seaborn as sns
titanic = sns.load_dataset('titanic')
print(titanic.head(10))
```

       survived  pclass     sex   age  sibsp  parch     fare embarked   class  \
    0         0       3    male  22.0      1      0   7.2500        S   Third   
    1         1       1  female  38.0      1      0  71.2833        C   First   
    2         1       3  female  26.0      0      0   7.9250        S   Third   
    3         1       1  female  35.0      1      0  53.1000        S   First   
    4         0       3    male  35.0      0      0   8.0500        S   Third   
    5         0       3    male   NaN      0      0   8.4583        Q   Third   
    6         0       1    male  54.0      0      0  51.8625        S   First   
    7         0       3    male   2.0      3      1  21.0750        S   Third   
    8         1       3  female  27.0      0      2  11.1333        S   Third   
    9         1       2  female  14.0      1      0  30.0708        C  Second   
    
         who  adult_male deck  embark_town alive  alone  
    0    man        True  NaN  Southampton    no  False  
    1  woman       False    C    Cherbourg   yes  False  
    2  woman       False  NaN  Southampton   yes   True  
    3  woman       False    C  Southampton   yes  False  
    4    man        True  NaN  Southampton    no   True  
    5    man        True  NaN   Queenstown    no   True  
    6    man        True    E  Southampton    no   True  
    7  child       False  NaN  Southampton    no  False  
    8  woman       False  NaN  Southampton   yes  False  
    9  child       False  NaN    Cherbourg   yes  False  
    


```python
# 히스토그램
sns.histplot(data= titanic, x = 'age',bins=10, hue= 'alive',multiple='stack')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7faf86744d10>




    
![png](images/day0706/output_5_1.png)
    



```python
# 확률밀도추정(KDE) 함수 그래프- 히스토그램을 부드러운 곡선 형태로 표현한다.
# 연속형 데이터 1개만 쓸 때 사용, y축은 수량의 비율
sns.kdeplot(data =titanic, x ='age',hue= 'alive',multiple='stack')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7faf856ddf50>




    
![png](images/day0706/output_6_1.png)
    


- 분포도
 - 수치형 데이터 한개 컬럼의 분포를 나타내는 그래프
  + 정규분포인가?
- ditstplot()히스토그램에 kdeplot와,rugplot을 한번에 그림


```python
sns.displot(data =titanic, x ='age')
```




    <seaborn.axisgrid.FacetGrid at 0x7faf85734850>




    
![png](images/day0706/output_8_1.png)
    



```python
sns.displot(data =titanic, x ='age',kind='kde')
```




    <seaborn.axisgrid.FacetGrid at 0x7faf85590050>




    
![png](images/day0706/output_9_1.png)
    



```python
sns.displot(data =titanic, x ='age',kde =True)
```




    <seaborn.axisgrid.FacetGrid at 0x7faf85621910>




    
![png](images/day0706/output_10_1.png)
    


## 범주형 데이터 시각화
- x축 범주형, y축 수치 데이터


```python
# 막대 그래프-matplotlib은 개수를 세는 작업을 해줘야하지만 seaborn은 알아서 해준다.
sns.barplot(x='class',y='fare',data= titanic)
# 클래스 별로 가격을 표시했지만 그 가격은 평균치를 나타내고 에러바(오차막대)를 만들어줌
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7faf8540f810>




    
![png](images/day0706/output_12_1.png)
    



```python
# 포인트 플롯
sns.pointplot(x='class',y='fare',data=titanic)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7faf853f1890>




    
![png](images/day0706/output_13_1.png)
    



```python
# boxplot(박스플롯)
sns.boxplot(x='class',y='age',data=titanic)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7faf8535b1d0>




    
![png](images/day0706/output_14_1.png)
    



```python
#바이올린 플롯
sns.violinplot(x= 'class', y='age', hue='sex',data=titanic,split =True)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7faf8527f590>




    
![png](images/day0706/output_15_1.png)
    



```python
# 카운트 플롯
# - 범주형데이터의 갯수 확인할 때 사용
sns.countplot(x = 'alive', data= titanic)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7faf852701d0>




    
![png](images/day0706/output_16_1.png)
    


## 데이터 관계 시각화
- 여러 데이터 사이의 관계도 파악을 위한 그래


```python
# 히트맵
flights =sns.load_dataset('flights')
print(flights.head(7))

```

       year month  passengers
    0  1949   Jan         112
    1  1949   Feb         118
    2  1949   Mar         132
    3  1949   Apr         129
    4  1949   May         121
    5  1949   Jun         135
    6  1949   Jul         148
    


```python
#  각 연도별 월별 승객수 구하기
# flights['year'].value_count()
flights_pivot = flights.pivot(index='month', columns='year', values='passengers')
print(flights_pivot)
```

    year   1949  1950  1951  1952  1953  1954  1955  1956  1957  1958  1959  1960
    month                                                                        
    Jan     112   115   145   171   196   204   242   284   315   340   360   417
    Feb     118   126   150   180   196   188   233   277   301   318   342   391
    Mar     132   141   178   193   236   235   267   317   356   362   406   419
    Apr     129   135   163   181   235   227   269   313   348   348   396   461
    May     121   125   172   183   229   234   270   318   355   363   420   472
    Jun     135   149   178   218   243   264   315   374   422   435   472   535
    Jul     148   170   199   230   264   302   364   413   465   491   548   622
    Aug     148   170   199   242   272   293   347   405   467   505   559   606
    Sep     136   158   184   209   237   259   312   355   404   404   463   508
    Oct     119   133   162   191   211   229   274   306   347   359   407   461
    Nov     104   114   146   172   180   203   237   271   305   310   362   390
    Dec     118   140   166   194   201   229   278   306   336   337   405   432
    


```python
sns.heatmap(data = flights_pivot)# 시각화를 통해 1960년대 8월에 가장 사람들 수가 많았다.
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7faf851e8610>




    
![png](images/day0706/output_20_1.png)
    



```python
# 라인플롯
sns.lineplot(x='year',y='passengers',data=flights)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7faf85117f10>




    
![png](images/day0706/output_21_1.png)
    



```python
# 산점도
tips =sns.load_dataset('tips')
tips.head(8)
# 영수증금액/ 팁/ 성별/ 담배/ 요일/ 시간/ 같이먹은 사람의 수(카운트데이터)
```





  <div id="df-98401926-69e1-464f-9f4f-ce6a7afd0680">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>total_bill</th>
      <th>tip</th>
      <th>sex</th>
      <th>smoker</th>
      <th>day</th>
      <th>time</th>
      <th>size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>16.99</td>
      <td>1.01</td>
      <td>Female</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10.34</td>
      <td>1.66</td>
      <td>Male</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>21.01</td>
      <td>3.50</td>
      <td>Male</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>23.68</td>
      <td>3.31</td>
      <td>Male</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>24.59</td>
      <td>3.61</td>
      <td>Female</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>4</td>
    </tr>
    <tr>
      <th>5</th>
      <td>25.29</td>
      <td>4.71</td>
      <td>Male</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>4</td>
    </tr>
    <tr>
      <th>6</th>
      <td>8.77</td>
      <td>2.00</td>
      <td>Male</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>2</td>
    </tr>
    <tr>
      <th>7</th>
      <td>26.88</td>
      <td>3.12</td>
      <td>Male</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-98401926-69e1-464f-9f4f-ce6a7afd0680')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-98401926-69e1-464f-9f4f-ce6a7afd0680 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-98401926-69e1-464f-9f4f-ce6a7afd0680');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python
# 두개의 연속형 데이터
sns.scatterplot(x='total_bill',y='tip',data= tips)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7faf8564f0d0>




    
![png](images/day0706/output_23_1.png)
    



```python
sns.scatterplot(x='total_bill',y='tip',hue ='time',data= tips)
# 저녁에 더 많은 팁을 준다.
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7faf854beed0>




    
![png](images/day0706/output_24_1.png)
    



```python
sns.scatterplot(x='total_bill',y='tip',hue ='sex',data= tips)
# 남자들의 경우 영수증 금액이 클수록 팁의 양(+)에 상관관계가 있다.
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7faf863f4750>




    
![png](images/day0706/output_25_1.png)
    



```python
# 회귀선
#선형회귀 적합선(상관관계를 표현한 선)을 포함한 산점도를 그리자.
sns.regplot(x= 'total_bill',y= 'tip', data =tips)
# 30달라를 냈을 때 4달러의 팁이 예상됨.
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7faf866085d0>




    
![png](images/day0706/output_26_1.png)
    



```python
sns.countplot(x='sex',data =tips)
# 남자와 여자를 비교할 때 남자들이 자주 팁을 준다.
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7faf867a81d0>




    
![png](images/day0706/output_27_1.png)
    



```python
sns.countplot(x='sex',hue='time',data=tips)
# 남자가 여자보다 저녁시간에 팁주는 횟수가 많다.
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7faf86796b10>




    
![png](images/day0706/output_28_1.png)
    



```python
sns.relplot(x="total_bill",y="tip",hue='size',data=tips)
```




    <seaborn.axisgrid.FacetGrid at 0x7faf8692f450>




    
![png](images/day0706/output_29_1.png)
    


## 머신러닝 리뷰
- 가장 인기 있는 모델
 + lightGBM, XGBoost

###  선형회귀
- 선형회귀식을 찾는 것이 중요
- $y =3 x +4$ 에 근사한 데이터 50개 생성


```python
import numpy as np 
import pandas as pd

# 시드값 고정- 랜덤한 내용이 다른 사람들과 같은 결과를 얻기 위해
np.random.seed(0)
intercept = 4 # 절편, 상수
slope = 3 # 기울기

# 변동성 주기 위해 노이즈 생성
noise = np.random.randn(50, 1)
x = 5 * np.random.rand(50, 1) # 0과 5사이의 실숫값 50개 생성
y = slope * x + intercept + noise

# 데이터 프레임 생성
data = pd.DataFrame({'X' : x[:, 0], 'Y' : y[:, 0]})
print(data)
```

               X          Y
    0   0.794848   8.148596
    1   0.551876   6.055784
    2   3.281648  14.823682
    3   0.690915   8.313637
    4   0.982912   8.816293
    5   1.843626   8.553600
    6   4.104966  17.264987
    7   0.485506   5.305162
    8   4.189725  16.465955
    9   0.480492   5.852075
    10  4.882297  18.790936
    11  2.343256  12.484042
    12  4.883805  19.412454
    13  3.024228  13.194358
    14  3.696318  15.532817
    15  0.195939   4.921491
    16  1.414035   9.736184
    17  0.600983   5.597790
    18  1.480701   8.755171
    19  0.593639   4.926820
    20  1.589916   6.216758
    21  2.071315  10.867564
    22  0.320737   5.826649
    23  3.462361  13.644917
    24  2.833007  14.768776
    25  1.326947   6.526477
    26  2.616240  11.894479
    27  0.469703   5.221924
    28  2.879732  14.171977
    29  4.646481  19.408802
    30  1.592845   8.933482
    31  3.337052  14.389318
    32  0.658989   5.089182
    33  3.581636  12.764112
    34  1.447030   7.993179
    35  0.915957   6.904219
    36  2.932565  14.027985
    37  0.100538   5.503993
    38  4.144700  16.046774
    39  0.023477   3.768129
    40  3.389083  13.118695
    41  1.350040   6.630102
    42  3.675970  13.321640
    43  4.810943  20.383604
    44  1.243766   7.221645
    45  2.880787  12.204286
    46  2.960210  11.627834
    47  2.861260  13.361269
    48  1.115408   5.732327
    49  4.763745  18.078495
    


```python
import matplotlib.pyplot as plt
fig, ax = plt.subplots()
ax.scatter(data['X'], data['Y'])
plt.show()
```


    
![png](images/day0706/output_33_0.png)
    



```python
import seaborn as sns 
sns.scatterplot(x = 'X', y = 'Y', data = data)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7faf85003c10>




    
![png](images/day0706/output_34_1.png)
    


#### 선형회귀 모형 훈련
- 모형 생성 후, 회귀계수 3과 y절편 4에 근사한 값이 나와야 함



```python
from sklearn import linear_model
from sklearn.linear_model import LinearRegression
lr_model = LinearRegression()#,선형회귀 모델
lr_model.fit(x,y)##모델훈련

print('y절편:', lr_model.intercept_)
print('회귀계수:', lr_model.coef_)
```

    y절편: [4.05757639]
    회귀계수: [[3.03754061]]
    


```python
# 예측값
y_pred = lr_model.predict(x)
fig, ax = plt.subplots()
ax.scatter(x, y)
ax.plot(x, y_pred, color='green')

# slope, intercept 
label = 'slope: {}\nintercept: {}'.format(round(lr_model.coef_[0][0], 2), round(lr_model.intercept_[0], 2))
ax.text(3.5, 4, label, style ='italic', 
        fontsize = 10, color ="green")
plt.show()
```


    
![png](images/day0706/output_37_0.png)
    


### 로지스틱 회귀
- 


```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(arr, scale=1):
    arr = np.asarray(arr)
    result = 1/(1 + np.exp(-arr*scale))
    return result

x = np.linspace(-6, 6)
y = sigmoid(x)

fig, ax = plt.subplots()
ax.plot(x, y)
ax.grid(which='major', axis='y', linestyle='--')
ax.axvline(x=0, color='r', linestyle='--', linewidth=1)
ax.set_ylim(0,1)
ax.set_yticks([0, 1, 0.5])
ax.text(0-0.1, 0.5, '0.5', ha='right')
ax.set_title('Sigmoid Graph')
plt.show()

```


    
![png](images/day0706/output_39_0.png)
    



```python
# 라이브러리 불러오기
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# 데이터 가져오기
x = np.arange(10).reshape(-1, 1)
y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

# 모델 생성 및 학습
model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)
model.fit(x, y)
```




    LogisticRegression(C=10.0, random_state=0, solver='liblinear')




```python
# 모형 평가
p_pred =model.predict_proba(x)
print('p_pred',p_pred,sep ='\n')
```

    p_pred
    [[0.97979027 0.02020973]
     [0.94958202 0.05041798]
     [0.87976149 0.12023851]
     [0.73975066 0.26024934]
     [0.52477284 0.47522716]
     [0.30020373 0.69979627]
     [0.1428487  0.8571513 ]
     [0.06080627 0.93919373]
     [0.02453462 0.97546538]
     [0.00967652 0.99032348]]
    


```python
y_pred = model.predict(x)
print('y_pred',y_pred)
```

    y_pred [0 0 0 0 0 1 1 1 1 1]
    


```python
fig, ax = plt.subplots()
ax.scatter(x, y)
ax.plot(x, p_pred[:, 1], color = 'black',  marker='o', markersize=6)
ax.plot()

ax.set_xticks(x)
ax.set_yticks(np.arange(0, 1.1, 0.1))

ax.grid(which='major', alpha=0.5)
plt.show()
```


    
![png](images/day0706/output_43_0.png)
    



```python
conf_m = confusion_matrix(y,y_pred)
print(conf_m)
```

    [[5 0]
     [0 5]]
    


```python
cm = confusion_matrix(y, y_pred)

fig, ax = plt.subplots(figsize=(8, 8))
ax.imshow(cm, cmap = 'Pastel1')# pastel2는 색깔임 'GnBu'
ax.grid(False)
ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0', 'Predicted 1'))
ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0', 'Actual 1'))
ax.set_ylim(1.5, -0.5)
for i in range(2):
    for j in range(2):
        ax.text(j, i, cm[i, j], ha='center', va='center', color='black', fontsize=20)
plt.show()
```


    
![png](images/day0706/output_45_0.png)
    


### 결정 트리
- 분류와 회귀 문제에 모두 사용가능

### 주요 개념
- 작동 원리
  + 데이터를 가장 잘 구분하는 조건을 정함.
  + 조건을 기준으로 데이터를 두 범주로 나눔
  + 나뉜 각 범주의 데이터를 구분하는 조건을 정함
  + 각 조건을 기준으로 데이터를 두 범주로 나눔
  + 언제까지 계속 분할할지 정한 후, 최종 결정 값을 구함.
- 불순도(Impurity)
  + 한 범주 안에 서로 다른 데이터가 얼마나 섞여 있는지 나타냄
  + 흰색과 검은색이 50:50으로 섞여 있다. (불순도 최대)
  + 흰색과 검은색으로 완전 분리 되었다. (불순도 최소)
- 엔트로피(Entropy)
  + 불확실한 정도를 의미함. 0 ~ 1로 정함.
  + 흰색과 검은색이 50:50으로 섞여 있다. 엔트로피 1
  + 흰색과 검은색으로 완전 분리 되었다. 엔트로피 0
- 정보이득(Information Gain)
  + 1에서 엔트로피를 뺀 수치
  + 정보 이득을 최대화하는 방향(엔트로피를 최소화 하는 방향)으로 노드를 분할함
- 지니 불순도(Gini Impurity)
  + 지니 불순도 값이 클수록 불순도도 높고, 작을수록 불순도도 낮음. 엔트로피와 마찬가지로 지니 불순도가 낮아지는 방향으로 노드 분할함.


```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
import seaborn as sns 

# tips 데이터셋 
titanic = sns.load_dataset('titanic')
titanic.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 891 entries, 0 to 890
    Data columns (total 15 columns):
     #   Column       Non-Null Count  Dtype   
    ---  ------       --------------  -----   
     0   survived     891 non-null    int64   
     1   pclass       891 non-null    int64   
     2   sex          891 non-null    object  
     3   age          714 non-null    float64 
     4   sibsp        891 non-null    int64   
     5   parch        891 non-null    int64   
     6   fare         891 non-null    float64 
     7   embarked     889 non-null    object  
     8   class        891 non-null    category
     9   who          891 non-null    object  
     10  adult_male   891 non-null    bool    
     11  deck         203 non-null    category
     12  embark_town  889 non-null    object  
     13  alive        891 non-null    object  
     14  alone        891 non-null    bool    
    dtypes: bool(2), category(2), float64(2), int64(4), object(5)
    memory usage: 80.7+ KB
    

- suvived의 비율을 구한다
 + 0: 사망자
 + 1: 생존


```python
titanic['survived'].value_counts()
```




    0    549
    1    342
    Name: survived, dtype: int64




```python
X = titanic[['pclass', 'parch', 'fare']]
y = titanic['survived']

# 훈련데이터, 테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape
```




    ((623, 3), (268, 3), (623,), (268,))




```python
tree_model = DecisionTreeClassifier()
tree_model.fit(X_train, y_train)

acc = tree_model.score(X_test, y_test)
print(f'모형 정확도 : {acc:.3f}') # 정확도 측정
```

    모형 정확도 : 0.675
    

## 랜덤 포레스


```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split 
import seaborn as sns 

# tips 데이터셋 
titanic = sns.load_dataset('titanic')

X = titanic[['pclass', 'parch', 'fare']]
y = titanic['survived']

# 훈련데이터, 테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state=42)

# 모델 훈련
rf_model = RandomForestClassifier(random_state=42) # 랜덤 포레스트 정의
rf_model.fit(X_train, y_train)

acc = rf_model.score(X_test, y_test)
print(f'모형 정확도 : {acc:.3f}') # 정확도 측정
```

    모형 정확도 : 0.675
    

## XGBoost(2016) & LightGBM
- 전통적인 머신러닝 알고리즘의 융합
  - 선형회귀 릿지 라쏘, 과적합 방지를 위한 규제
  + 결정트리의 핵심적인 알고리즘
  + 경사 하강법
  + 부스팅 기법
- 문제점 : 파라미터의 개수가 매우 많음
- 왜 많이 사용할까?
 + 모델 학습 속도
 + 성능
 + 가장 좋은 모델이란 학습속도는 빠르고 성능이 좋음.(기준: 지금까지 나온 알고리즘과 비교해서)
- 언어를 Python,Java에서 시작했어도 C,C++로 가야만 됨.
- 개발 초기: 자체 사용 용도로 개발 --> Python Wrapper 
  - R, 머신러닝 프레임워크 종류가 다양.
  - 파이썬 머신러닝 중 Scikit-Learn이 대세로 떠오름
- 개발 중기: 파이썬 머신러닝 Scikit-Learn에서 API를 사용해 XGBoost을 사용


### XGBoost-Python Wrapper 방식
- X_train, Y_train
- 각 모듈에 맞도록 행렬을 재변환해야 함.




```python
import xgboost as xgb # 엑스지부스터를 사용->파이썬래퍼 방식
from sklearn.model_selection import train_test_split
import seaborn as sns
```


```python
# 데이터 분리
titanic = sns.load_dataset('titanic')
titanic.info()

#x,독립변수 y종속변수
X = titanic[['pclass', 'parch', 'fare']]
y = titanic['survived']

# 훈련데이터, 테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    stratify = y, 
                                                    test_size = 0.3, 
                                                    random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 891 entries, 0 to 890
    Data columns (total 15 columns):
     #   Column       Non-Null Count  Dtype   
    ---  ------       --------------  -----   
     0   survived     891 non-null    int64   
     1   pclass       891 non-null    int64   
     2   sex          891 non-null    object  
     3   age          714 non-null    float64 
     4   sibsp        891 non-null    int64   
     5   parch        891 non-null    int64   
     6   fare         891 non-null    float64 
     7   embarked     889 non-null    object  
     8   class        891 non-null    category
     9   who          891 non-null    object  
     10  adult_male   891 non-null    bool    
     11  deck         203 non-null    category
     12  embark_town  889 non-null    object  
     13  alive        891 non-null    object  
     14  alone        891 non-null    bool    
    dtypes: bool(2), category(2), float64(2), int64(4), object(5)
    memory usage: 80.7+ KB
    




    ((623, 3), (268, 3), (623,), (268,))



- **여기가 핵심**

   + 다른 데이터로 변신




```python
dtrain = xgb.DMatrix(data = X_train, label = y_train)
dtest = xgb.DMatrix(data= X_test, label= y_test)

print(dtrain)
```

    <xgboost.core.DMatrix object at 0x7faf7bee2490>
    


```python
params = {
    'max_depth':3,#트리 깊이는 3
    'n_estimator':100,#100번 심기->결정트리갯수
    'eta':0.1,
    'objectice' : 'binary:logistic'
}
num_rounds = 400

w_list = [(dtrain, 'train'), (dtest, 'test')]
xgb_ml = xgb.train(params = params, 
                   dtrain = dtrain, 
                   num_boost_round = 400, #경사하강법의 에포크(가중치를 계속 줌)
                   early_stopping_rounds = 100, # 100번하고 효과 없으면 멈춰라
                   evals = w_list)
```

    [0]	train-rmse:0.487916	test-rmse:0.490147
    Multiple eval metrics have been passed: 'test-rmse' will be used for early stopping.
    
    Will train until test-rmse hasn't improved in 100 rounds.
    [1]	train-rmse:0.477856	test-rmse:0.482183
    [2]	train-rmse:0.470061	test-rmse:0.47532
    [3]	train-rmse:0.462181	test-rmse:0.470924
    [4]	train-rmse:0.455721	test-rmse:0.466351
    [5]	train-rmse:0.450355	test-rmse:0.462927
    [6]	train-rmse:0.445684	test-rmse:0.459554
    [7]	train-rmse:0.441298	test-rmse:0.458577
    [8]	train-rmse:0.437792	test-rmse:0.456831
    [9]	train-rmse:0.434393	test-rmse:0.456526
    [10]	train-rmse:0.431725	test-rmse:0.455362
    [11]	train-rmse:0.428592	test-rmse:0.454746
    [12]	train-rmse:0.426484	test-rmse:0.454121
    [13]	train-rmse:0.423993	test-rmse:0.453891
    [14]	train-rmse:0.421947	test-rmse:0.453115
    [15]	train-rmse:0.42036	test-rmse:0.452995
    [16]	train-rmse:0.418511	test-rmse:0.452991
    [17]	train-rmse:0.417025	test-rmse:0.452558
    [18]	train-rmse:0.415955	test-rmse:0.45293
    [19]	train-rmse:0.41396	test-rmse:0.453367
    [20]	train-rmse:0.413199	test-rmse:0.453852
    [21]	train-rmse:0.412091	test-rmse:0.453547
    [22]	train-rmse:0.410501	test-rmse:0.454099
    [23]	train-rmse:0.409461	test-rmse:0.453524
    [24]	train-rmse:0.408469	test-rmse:0.453724
    [25]	train-rmse:0.407781	test-rmse:0.454117
    [26]	train-rmse:0.406954	test-rmse:0.454325
    [27]	train-rmse:0.405709	test-rmse:0.454997
    [28]	train-rmse:0.405121	test-rmse:0.455544
    [29]	train-rmse:0.40445	test-rmse:0.455746
    [30]	train-rmse:0.403643	test-rmse:0.45576
    [31]	train-rmse:0.403092	test-rmse:0.45603
    [32]	train-rmse:0.40252	test-rmse:0.456502
    [33]	train-rmse:0.401617	test-rmse:0.456903
    [34]	train-rmse:0.401175	test-rmse:0.457341
    [35]	train-rmse:0.400151	test-rmse:0.458455
    [36]	train-rmse:0.399748	test-rmse:0.458725
    [37]	train-rmse:0.398984	test-rmse:0.45933
    [38]	train-rmse:0.3982	test-rmse:0.459086
    [39]	train-rmse:0.397529	test-rmse:0.459736
    [40]	train-rmse:0.39734	test-rmse:0.460037
    [41]	train-rmse:0.396627	test-rmse:0.460473
    [42]	train-rmse:0.396461	test-rmse:0.460603
    [43]	train-rmse:0.395536	test-rmse:0.460408
    [44]	train-rmse:0.395255	test-rmse:0.460791
    [45]	train-rmse:0.394568	test-rmse:0.461165
    [46]	train-rmse:0.39406	test-rmse:0.461553
    [47]	train-rmse:0.39335	test-rmse:0.461595
    [48]	train-rmse:0.393206	test-rmse:0.461718
    [49]	train-rmse:0.392991	test-rmse:0.462002
    [50]	train-rmse:0.392352	test-rmse:0.461921
    [51]	train-rmse:0.391973	test-rmse:0.462235
    [52]	train-rmse:0.391844	test-rmse:0.462413
    [53]	train-rmse:0.391345	test-rmse:0.462504
    [54]	train-rmse:0.391184	test-rmse:0.462824
    [55]	train-rmse:0.391068	test-rmse:0.462939
    [56]	train-rmse:0.390596	test-rmse:0.462162
    [57]	train-rmse:0.390164	test-rmse:0.462743
    [58]	train-rmse:0.389861	test-rmse:0.463045
    [59]	train-rmse:0.389441	test-rmse:0.462628
    [60]	train-rmse:0.389338	test-rmse:0.462737
    [61]	train-rmse:0.388745	test-rmse:0.462943
    [62]	train-rmse:0.388405	test-rmse:0.462691
    [63]	train-rmse:0.388277	test-rmse:0.463041
    [64]	train-rmse:0.387828	test-rmse:0.463243
    [65]	train-rmse:0.387614	test-rmse:0.463214
    [66]	train-rmse:0.387088	test-rmse:0.463584
    [67]	train-rmse:0.386906	test-rmse:0.463627
    [68]	train-rmse:0.386444	test-rmse:0.463517
    [69]	train-rmse:0.385685	test-rmse:0.463735
    [70]	train-rmse:0.385353	test-rmse:0.463113
    [71]	train-rmse:0.384828	test-rmse:0.463005
    [72]	train-rmse:0.38444	test-rmse:0.462966
    [73]	train-rmse:0.383198	test-rmse:0.462825
    [74]	train-rmse:0.382841	test-rmse:0.462947
    [75]	train-rmse:0.382416	test-rmse:0.463253
    [76]	train-rmse:0.381966	test-rmse:0.462963
    [77]	train-rmse:0.381655	test-rmse:0.463399
    [78]	train-rmse:0.381399	test-rmse:0.463326
    [79]	train-rmse:0.380593	test-rmse:0.463278
    [80]	train-rmse:0.380329	test-rmse:0.463051
    [81]	train-rmse:0.380233	test-rmse:0.46316
    [82]	train-rmse:0.379942	test-rmse:0.463234
    [83]	train-rmse:0.379686	test-rmse:0.463517
    [84]	train-rmse:0.37893	test-rmse:0.463051
    [85]	train-rmse:0.37884	test-rmse:0.463287
    [86]	train-rmse:0.378756	test-rmse:0.463389
    [87]	train-rmse:0.378501	test-rmse:0.463446
    [88]	train-rmse:0.378011	test-rmse:0.463085
    [89]	train-rmse:0.377178	test-rmse:0.46281
    [90]	train-rmse:0.376872	test-rmse:0.462851
    [91]	train-rmse:0.376563	test-rmse:0.46331
    [92]	train-rmse:0.376317	test-rmse:0.463088
    [93]	train-rmse:0.376049	test-rmse:0.463515
    [94]	train-rmse:0.375914	test-rmse:0.463486
    [95]	train-rmse:0.375643	test-rmse:0.463194
    [96]	train-rmse:0.375378	test-rmse:0.463459
    [97]	train-rmse:0.375145	test-rmse:0.46375
    [98]	train-rmse:0.37453	test-rmse:0.463309
    [99]	train-rmse:0.374021	test-rmse:0.463431
    [100]	train-rmse:0.373289	test-rmse:0.463593
    [101]	train-rmse:0.373032	test-rmse:0.463798
    [102]	train-rmse:0.372814	test-rmse:0.464226
    [103]	train-rmse:0.372443	test-rmse:0.464454
    [104]	train-rmse:0.372217	test-rmse:0.464417
    [105]	train-rmse:0.372146	test-rmse:0.464435
    [106]	train-rmse:0.371726	test-rmse:0.464241
    [107]	train-rmse:0.37158	test-rmse:0.464172
    [108]	train-rmse:0.371396	test-rmse:0.464582
    [109]	train-rmse:0.371335	test-rmse:0.4646
    [110]	train-rmse:0.371127	test-rmse:0.464403
    [111]	train-rmse:0.371001	test-rmse:0.464328
    [112]	train-rmse:0.370928	test-rmse:0.464546
    [113]	train-rmse:0.370688	test-rmse:0.464457
    [114]	train-rmse:0.370634	test-rmse:0.464475
    [115]	train-rmse:0.370421	test-rmse:0.464819
    [116]	train-rmse:0.37007	test-rmse:0.46479
    [117]	train-rmse:0.369922	test-rmse:0.464798
    Stopping. Best iteration:
    [17]	train-rmse:0.417025	test-rmse:0.452558
    
    


```python
#평가
from sklearn.metrics import accuracy_score
pred_probs = xgb_ml.predict(dtest)
y_pred = [1 if x > 0.5 else 0 for x in pred_probs]

# 예측 라벨과 실제 라벨 사이의 정확도 측정
accuracy_score(y_pred,y_test)
```




    0.6977611940298507



### XGBoost Scikit-Learn API방식
- application programming interface


```python
#xgb를 사용하지 않음(파이썬라이브러리를 가져오지 않고 사이킥런인공지능 사용)
#from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier # 사이킥런 API

#dt = DecisionTreeClassifier()
params = {
    'max_depth':3,#트리 깊이는 3
    'n_estimator':100,#100번 심기->결정트리갯수
    'eta':0.1,
    'objectice' : 'binary:logistic'
}
num_rounds = 400

xgb_model = XGBClassifier(objective= 'binary:logistic',
                          n_estimators=100,
                          max_depth=3,
                          learning_rate =0.1,
                          num_rounds = 400,#이건 엑스지부스터랑 같은내용이지만 양식이 다름
                          random_state = 42)

w_list = [(X_train, y_train), (X_test, y_test)]

xgb_model.fit(X_train, y_train, eval_set = w_list, eval_metric='error', verbose=True)

y_probas = xgb_model.predict_proba(X_test)
y_pred = [1 if x > 0.5 else 0 for x in pred_probs]

# 예측 라벨과 실제 라벨 사이의 정확도 측정
accuracy_score(y_pred, y_test)

```

    [0]	validation_0-error:0.260032	validation_1-error:0.302239
    [1]	validation_0-error:0.260032	validation_1-error:0.302239
    [2]	validation_0-error:0.260032	validation_1-error:0.302239
    [3]	validation_0-error:0.260032	validation_1-error:0.302239
    [4]	validation_0-error:0.260032	validation_1-error:0.302239
    [5]	validation_0-error:0.260032	validation_1-error:0.302239
    [6]	validation_0-error:0.260032	validation_1-error:0.302239
    [7]	validation_0-error:0.260032	validation_1-error:0.302239
    [8]	validation_0-error:0.260032	validation_1-error:0.302239
    [9]	validation_0-error:0.260032	validation_1-error:0.302239
    [10]	validation_0-error:0.260032	validation_1-error:0.302239
    [11]	validation_0-error:0.260032	validation_1-error:0.302239
    [12]	validation_0-error:0.260032	validation_1-error:0.302239
    [13]	validation_0-error:0.247191	validation_1-error:0.298507
    [14]	validation_0-error:0.247191	validation_1-error:0.298507
    [15]	validation_0-error:0.248796	validation_1-error:0.302239
    [16]	validation_0-error:0.248796	validation_1-error:0.302239
    [17]	validation_0-error:0.248796	validation_1-error:0.302239
    [18]	validation_0-error:0.248796	validation_1-error:0.302239
    [19]	validation_0-error:0.248796	validation_1-error:0.302239
    [20]	validation_0-error:0.248796	validation_1-error:0.302239
    [21]	validation_0-error:0.248796	validation_1-error:0.302239
    [22]	validation_0-error:0.248796	validation_1-error:0.302239
    [23]	validation_0-error:0.248796	validation_1-error:0.302239
    [24]	validation_0-error:0.248796	validation_1-error:0.302239
    [25]	validation_0-error:0.248796	validation_1-error:0.302239
    [26]	validation_0-error:0.248796	validation_1-error:0.302239
    [27]	validation_0-error:0.248796	validation_1-error:0.302239
    [28]	validation_0-error:0.247191	validation_1-error:0.302239
    [29]	validation_0-error:0.247191	validation_1-error:0.302239
    [30]	validation_0-error:0.247191	validation_1-error:0.302239
    [31]	validation_0-error:0.243981	validation_1-error:0.298507
    [32]	validation_0-error:0.247191	validation_1-error:0.302239
    [33]	validation_0-error:0.243981	validation_1-error:0.298507
    [34]	validation_0-error:0.243981	validation_1-error:0.298507
    [35]	validation_0-error:0.242376	validation_1-error:0.294776
    [36]	validation_0-error:0.24077	validation_1-error:0.294776
    [37]	validation_0-error:0.24077	validation_1-error:0.294776
    [38]	validation_0-error:0.24077	validation_1-error:0.294776
    [39]	validation_0-error:0.24077	validation_1-error:0.294776
    [40]	validation_0-error:0.24077	validation_1-error:0.294776
    [41]	validation_0-error:0.24077	validation_1-error:0.294776
    [42]	validation_0-error:0.24077	validation_1-error:0.294776
    [43]	validation_0-error:0.24077	validation_1-error:0.294776
    [44]	validation_0-error:0.24077	validation_1-error:0.302239
    [45]	validation_0-error:0.24077	validation_1-error:0.302239
    [46]	validation_0-error:0.24077	validation_1-error:0.302239
    [47]	validation_0-error:0.24077	validation_1-error:0.302239
    [48]	validation_0-error:0.24077	validation_1-error:0.302239
    [49]	validation_0-error:0.24077	validation_1-error:0.302239
    [50]	validation_0-error:0.24077	validation_1-error:0.302239
    [51]	validation_0-error:0.24077	validation_1-error:0.302239
    [52]	validation_0-error:0.23435	validation_1-error:0.302239
    [53]	validation_0-error:0.23435	validation_1-error:0.302239
    [54]	validation_0-error:0.232745	validation_1-error:0.298507
    [55]	validation_0-error:0.229535	validation_1-error:0.298507
    [56]	validation_0-error:0.229535	validation_1-error:0.298507
    [57]	validation_0-error:0.229535	validation_1-error:0.298507
    [58]	validation_0-error:0.229535	validation_1-error:0.298507
    [59]	validation_0-error:0.227929	validation_1-error:0.294776
    [60]	validation_0-error:0.227929	validation_1-error:0.298507
    [61]	validation_0-error:0.227929	validation_1-error:0.298507
    [62]	validation_0-error:0.227929	validation_1-error:0.298507
    [63]	validation_0-error:0.227929	validation_1-error:0.298507
    [64]	validation_0-error:0.227929	validation_1-error:0.298507
    [65]	validation_0-error:0.227929	validation_1-error:0.298507
    [66]	validation_0-error:0.227929	validation_1-error:0.298507
    [67]	validation_0-error:0.227929	validation_1-error:0.298507
    [68]	validation_0-error:0.227929	validation_1-error:0.298507
    [69]	validation_0-error:0.227929	validation_1-error:0.298507
    [70]	validation_0-error:0.227929	validation_1-error:0.298507
    [71]	validation_0-error:0.227929	validation_1-error:0.298507
    [72]	validation_0-error:0.227929	validation_1-error:0.302239
    [73]	validation_0-error:0.227929	validation_1-error:0.302239
    [74]	validation_0-error:0.229535	validation_1-error:0.30597
    [75]	validation_0-error:0.229535	validation_1-error:0.30597
    [76]	validation_0-error:0.229535	validation_1-error:0.30597
    [77]	validation_0-error:0.229535	validation_1-error:0.30597
    [78]	validation_0-error:0.229535	validation_1-error:0.30597
    [79]	validation_0-error:0.229535	validation_1-error:0.30597
    [80]	validation_0-error:0.229535	validation_1-error:0.30597
    [81]	validation_0-error:0.229535	validation_1-error:0.30597
    [82]	validation_0-error:0.229535	validation_1-error:0.30597
    [83]	validation_0-error:0.229535	validation_1-error:0.30597
    [84]	validation_0-error:0.229535	validation_1-error:0.30597
    [85]	validation_0-error:0.229535	validation_1-error:0.30597
    [86]	validation_0-error:0.229535	validation_1-error:0.30597
    [87]	validation_0-error:0.229535	validation_1-error:0.30597
    [88]	validation_0-error:0.229535	validation_1-error:0.30597
    [89]	validation_0-error:0.229535	validation_1-error:0.30597
    [90]	validation_0-error:0.229535	validation_1-error:0.30597
    [91]	validation_0-error:0.229535	validation_1-error:0.30597
    [92]	validation_0-error:0.229535	validation_1-error:0.30597
    [93]	validation_0-error:0.229535	validation_1-error:0.30597
    [94]	validation_0-error:0.227929	validation_1-error:0.313433
    [95]	validation_0-error:0.226324	validation_1-error:0.313433
    [96]	validation_0-error:0.223114	validation_1-error:0.317164
    [97]	validation_0-error:0.223114	validation_1-error:0.317164
    [98]	validation_0-error:0.223114	validation_1-error:0.317164
    [99]	validation_0-error:0.223114	validation_1-error:0.317164
    




    0.6977611940298507



### LightGBM Rython Wrapper방식



```python
import lightgbm as lgb 
from sklearn.model_selection import train_test_split 
from sklearn.metrics import accuracy_score
import seaborn as sns 

# tips 데이터셋 
titanic = sns.load_dataset('titanic')

X = titanic[['pclass', 'parch', 'fare']]
y = titanic['survived']

# 훈련데이터, 테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state=42)

# XGBoost 코드와 유사하다. 
dtrain = lgb.Dataset(data = X_train, label = y_train)
dtest = lgb.Dataset(data = X_test, label = y_test)

params = {'max_depth':3,
          'n_estimators':100,
          'learning_rate': 0.1, #xgbooost eta
          'objective':'binary',# xgboost objectice' : 'binary:logistic'
          'metric' : 'binary_error', 
          'num_boost_round' : 400, 
          'verbose' : 1} 

w_list = [dtrain, dtest]
lgb_ml = lgb.train(params=params, train_set = dtrain,\
                  early_stopping_rounds=100, valid_sets= w_list)

pred_probs = lgb_ml.predict(X_test)
y_pred=[1 if x > 0.5 else 0 for x in pred_probs]

# 예측 라벨과 실제 라벨 사이의 정확도 측정
accuracy_score(y_pred, y_test)
```

    /usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument
      warnings.warn("Found `{}` in params. Will use it instead of argument".format(alias))
    

    [1]	training's binary_error: 0.383628	valid_1's binary_error: 0.384328
    Training until validation scores don't improve for 100 rounds.
    [2]	training's binary_error: 0.383628	valid_1's binary_error: 0.384328
    [3]	training's binary_error: 0.354735	valid_1's binary_error: 0.369403
    [4]	training's binary_error: 0.29695	valid_1's binary_error: 0.354478
    [5]	training's binary_error: 0.272873	valid_1's binary_error: 0.33209
    [6]	training's binary_error: 0.272873	valid_1's binary_error: 0.33209
    [7]	training's binary_error: 0.269663	valid_1's binary_error: 0.317164
    [8]	training's binary_error: 0.269663	valid_1's binary_error: 0.317164
    [9]	training's binary_error: 0.264848	valid_1's binary_error: 0.309701
    [10]	training's binary_error: 0.269663	valid_1's binary_error: 0.309701
    [11]	training's binary_error: 0.264848	valid_1's binary_error: 0.309701
    [12]	training's binary_error: 0.264848	valid_1's binary_error: 0.309701
    [13]	training's binary_error: 0.264848	valid_1's binary_error: 0.309701
    [14]	training's binary_error: 0.264848	valid_1's binary_error: 0.309701
    [15]	training's binary_error: 0.264848	valid_1's binary_error: 0.309701
    [16]	training's binary_error: 0.266453	valid_1's binary_error: 0.313433
    [17]	training's binary_error: 0.266453	valid_1's binary_error: 0.313433
    [18]	training's binary_error: 0.266453	valid_1's binary_error: 0.313433
    [19]	training's binary_error: 0.266453	valid_1's binary_error: 0.313433
    [20]	training's binary_error: 0.266453	valid_1's binary_error: 0.313433
    [21]	training's binary_error: 0.266453	valid_1's binary_error: 0.313433
    [22]	training's binary_error: 0.266453	valid_1's binary_error: 0.313433
    [23]	training's binary_error: 0.271268	valid_1's binary_error: 0.313433
    [24]	training's binary_error: 0.258427	valid_1's binary_error: 0.309701
    [25]	training's binary_error: 0.258427	valid_1's binary_error: 0.309701
    [26]	training's binary_error: 0.258427	valid_1's binary_error: 0.309701
    [27]	training's binary_error: 0.258427	valid_1's binary_error: 0.309701
    [28]	training's binary_error: 0.258427	valid_1's binary_error: 0.309701
    [29]	training's binary_error: 0.255217	valid_1's binary_error: 0.309701
    [30]	training's binary_error: 0.255217	valid_1's binary_error: 0.309701
    [31]	training's binary_error: 0.255217	valid_1's binary_error: 0.309701
    [32]	training's binary_error: 0.255217	valid_1's binary_error: 0.309701
    [33]	training's binary_error: 0.255217	valid_1's binary_error: 0.317164
    [34]	training's binary_error: 0.255217	valid_1's binary_error: 0.317164
    [35]	training's binary_error: 0.255217	valid_1's binary_error: 0.317164
    [36]	training's binary_error: 0.255217	valid_1's binary_error: 0.309701
    [37]	training's binary_error: 0.255217	valid_1's binary_error: 0.317164
    [38]	training's binary_error: 0.255217	valid_1's binary_error: 0.317164
    [39]	training's binary_error: 0.248796	valid_1's binary_error: 0.309701
    [40]	training's binary_error: 0.248796	valid_1's binary_error: 0.313433
    [41]	training's binary_error: 0.248796	valid_1's binary_error: 0.313433
    [42]	training's binary_error: 0.248796	valid_1's binary_error: 0.313433
    [43]	training's binary_error: 0.248796	valid_1's binary_error: 0.313433
    [44]	training's binary_error: 0.248796	valid_1's binary_error: 0.313433
    [45]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [46]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [47]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [48]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [49]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [50]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [51]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [52]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [53]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [54]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [55]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [56]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [57]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [58]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [59]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [60]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [61]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [62]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [63]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [64]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [65]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [66]	training's binary_error: 0.243981	valid_1's binary_error: 0.309701
    [67]	training's binary_error: 0.23435	valid_1's binary_error: 0.309701
    [68]	training's binary_error: 0.23435	valid_1's binary_error: 0.309701
    [69]	training's binary_error: 0.23435	valid_1's binary_error: 0.309701
    [70]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [71]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [72]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [73]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [74]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [75]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [76]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [77]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [78]	training's binary_error: 0.232745	valid_1's binary_error: 0.313433
    [79]	training's binary_error: 0.232745	valid_1's binary_error: 0.313433
    [80]	training's binary_error: 0.232745	valid_1's binary_error: 0.313433
    [81]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [82]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [83]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [84]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [85]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [86]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [87]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [88]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [89]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [90]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [91]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [92]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [93]	training's binary_error: 0.227929	valid_1's binary_error: 0.309701
    [94]	training's binary_error: 0.227929	valid_1's binary_error: 0.309701
    [95]	training's binary_error: 0.227929	valid_1's binary_error: 0.309701
    [96]	training's binary_error: 0.227929	valid_1's binary_error: 0.309701
    [97]	training's binary_error: 0.227929	valid_1's binary_error: 0.309701
    [98]	training's binary_error: 0.227929	valid_1's binary_error: 0.309701
    [99]	training's binary_error: 0.221509	valid_1's binary_error: 0.317164
    [100]	training's binary_error: 0.227929	valid_1's binary_error: 0.309701
    [101]	training's binary_error: 0.23114	valid_1's binary_error: 0.30597
    [102]	training's binary_error: 0.23114	valid_1's binary_error: 0.30597
    [103]	training's binary_error: 0.227929	valid_1's binary_error: 0.309701
    [104]	training's binary_error: 0.221509	valid_1's binary_error: 0.317164
    [105]	training's binary_error: 0.221509	valid_1's binary_error: 0.317164
    [106]	training's binary_error: 0.224719	valid_1's binary_error: 0.313433
    [107]	training's binary_error: 0.224719	valid_1's binary_error: 0.317164
    [108]	training's binary_error: 0.224719	valid_1's binary_error: 0.317164
    [109]	training's binary_error: 0.224719	valid_1's binary_error: 0.317164
    [110]	training's binary_error: 0.224719	valid_1's binary_error: 0.317164
    [111]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [112]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [113]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [114]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [115]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [116]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [117]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [118]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [119]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [120]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [121]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [122]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [123]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [124]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [125]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [126]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [127]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [128]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [129]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [130]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [131]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [132]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [133]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [134]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [135]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [136]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [137]	training's binary_error: 0.219904	valid_1's binary_error: 0.309701
    [138]	training's binary_error: 0.219904	valid_1's binary_error: 0.309701
    [139]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [140]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [141]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [142]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [143]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [144]	training's binary_error: 0.221509	valid_1's binary_error: 0.320896
    [145]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [146]	training's binary_error: 0.221509	valid_1's binary_error: 0.313433
    [147]	training's binary_error: 0.221509	valid_1's binary_error: 0.313433
    [148]	training's binary_error: 0.221509	valid_1's binary_error: 0.313433
    [149]	training's binary_error: 0.221509	valid_1's binary_error: 0.313433
    [150]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [151]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [152]	training's binary_error: 0.221509	valid_1's binary_error: 0.313433
    [153]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [154]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [155]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [156]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [157]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [158]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [159]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [160]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [161]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [162]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [163]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [164]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [165]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [166]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [167]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [168]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [169]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [170]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [171]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [172]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [173]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [174]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [175]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [176]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [177]	training's binary_error: 0.221509	valid_1's binary_error: 0.328358
    [178]	training's binary_error: 0.221509	valid_1's binary_error: 0.328358
    [179]	training's binary_error: 0.221509	valid_1's binary_error: 0.328358
    [180]	training's binary_error: 0.221509	valid_1's binary_error: 0.328358
    [181]	training's binary_error: 0.221509	valid_1's binary_error: 0.328358
    [182]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [183]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [184]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [185]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [186]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [187]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [188]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [189]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [190]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [191]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [192]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [193]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [194]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [195]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [196]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [197]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [198]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [199]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [200]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [201]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    Early stopping, best iteration is:
    [101]	training's binary_error: 0.23114	valid_1's binary_error: 0.30597
    




    0.6940298507462687



-  https://lightgbm.readthedocs.io/en/latest/Parameters.html
-  파라메터를 보고 할 것

### LightGBM Scikit-Learn API방식
- application programming interface


```python
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score

# model 
w_list = [dtrain, dtest]
model = LGBMClassifier(objective = 'binary', 
                       metric = 'binary_error',
                       n_estimators=100, 
                       learning_rate=0.1, 
                       max_depth=3, 
                       num_boost_round = 400,
                       random_state = 32)
model.fit(X_train, 
          y_train, 
          eval_set = [(X_train, y_train), (X_test, y_test)], 
          verbose=1,
          early_stopping_rounds = 100)
y_probas = model.predict_proba(X_test) 
y_pred=[1 if x > 0.5 else 0 for x in y_probas[:, 1]] # 예측 라벨(0과 1로 예측)

# 예측 라벨과 실제 라벨 사이의 정확도 측정
accuracy_score(y_pred, y_test)
```

    [1]	training's binary_error: 0.383628	valid_1's binary_error: 0.384328
    Training until validation scores don't improve for 100 rounds.
    [2]	training's binary_error: 0.383628	valid_1's binary_error: 0.384328
    [3]	training's binary_error: 0.354735	valid_1's binary_error: 0.369403
    [4]	training's binary_error: 0.29695	valid_1's binary_error: 0.354478
    [5]	training's binary_error: 0.272873	valid_1's binary_error: 0.33209
    [6]	training's binary_error: 0.272873	valid_1's binary_error: 0.33209
    [7]	training's binary_error: 0.269663	valid_1's binary_error: 0.317164
    [8]	training's binary_error: 0.269663	valid_1's binary_error: 0.317164
    [9]	training's binary_error: 0.264848	valid_1's binary_error: 0.309701
    [10]	training's binary_error: 0.269663	valid_1's binary_error: 0.309701
    [11]	training's binary_error: 0.264848	valid_1's binary_error: 0.309701
    [12]	training's binary_error: 0.264848	valid_1's binary_error: 0.309701
    [13]	training's binary_error: 0.264848	valid_1's binary_error: 0.309701
    [14]	training's binary_error: 0.264848	valid_1's binary_error: 0.309701
    [15]	training's binary_error: 0.264848	valid_1's binary_error: 0.309701
    [16]	training's binary_error: 0.266453	valid_1's binary_error: 0.313433
    [17]	training's binary_error: 0.266453	valid_1's binary_error: 0.313433
    [18]	training's binary_error: 0.266453	valid_1's binary_error: 0.313433
    [19]	training's binary_error: 0.266453	valid_1's binary_error: 0.313433
    [20]	training's binary_error: 0.266453	valid_1's binary_error: 0.313433
    [21]	training's binary_error: 0.266453	valid_1's binary_error: 0.313433
    [22]	training's binary_error: 0.266453	valid_1's binary_error: 0.313433
    [23]	training's binary_error: 0.271268	valid_1's binary_error: 0.313433
    [24]	training's binary_error: 0.258427	valid_1's binary_error: 0.309701
    [25]	training's binary_error: 0.258427	valid_1's binary_error: 0.309701
    [26]	training's binary_error: 0.258427	valid_1's binary_error: 0.309701
    [27]	training's binary_error: 0.258427	valid_1's binary_error: 0.309701
    [28]	training's binary_error: 0.258427	valid_1's binary_error: 0.309701
    [29]	training's binary_error: 0.255217	valid_1's binary_error: 0.309701
    [30]	training's binary_error: 0.255217	valid_1's binary_error: 0.309701
    [31]	training's binary_error: 0.255217	valid_1's binary_error: 0.309701
    [32]	training's binary_error: 0.255217	valid_1's binary_error: 0.309701
    [33]	training's binary_error: 0.255217	valid_1's binary_error: 0.317164
    [34]	training's binary_error: 0.255217	valid_1's binary_error: 0.317164
    [35]	training's binary_error: 0.255217	valid_1's binary_error: 0.317164
    [36]	training's binary_error: 0.255217	valid_1's binary_error: 0.309701
    [37]	training's binary_error: 0.255217	valid_1's binary_error: 0.317164
    [38]	training's binary_error: 0.255217	valid_1's binary_error: 0.317164
    [39]	training's binary_error: 0.248796	valid_1's binary_error: 0.309701
    [40]	training's binary_error: 0.248796	valid_1's binary_error: 0.313433
    [41]	training's binary_error: 0.248796	valid_1's binary_error: 0.313433
    [42]	training's binary_error: 0.248796	valid_1's binary_error: 0.313433
    [43]	training's binary_error: 0.248796	valid_1's binary_error: 0.313433
    [44]	training's binary_error: 0.248796	valid_1's binary_error: 0.313433
    [45]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [46]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [47]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [48]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [49]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [50]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [51]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [52]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [53]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [54]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [55]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [56]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [57]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [58]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [59]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [60]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [61]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [62]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [63]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [64]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [65]	training's binary_error: 0.247191	valid_1's binary_error: 0.313433
    [66]	training's binary_error: 0.243981	valid_1's binary_error: 0.309701
    [67]	training's binary_error: 0.23435	valid_1's binary_error: 0.309701
    [68]	training's binary_error: 0.23435	valid_1's binary_error: 0.309701
    [69]	training's binary_error: 0.23435	valid_1's binary_error: 0.309701
    [70]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [71]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [72]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [73]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [74]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [75]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [76]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [77]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [78]	training's binary_error: 0.232745	valid_1's binary_error: 0.313433
    [79]	training's binary_error: 0.232745	valid_1's binary_error: 0.313433
    [80]	training's binary_error: 0.232745	valid_1's binary_error: 0.313433
    [81]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [82]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [83]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [84]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [85]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [86]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [87]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [88]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [89]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [90]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [91]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [92]	training's binary_error: 0.229535	valid_1's binary_error: 0.309701
    [93]	training's binary_error: 0.227929	valid_1's binary_error: 0.309701
    [94]	training's binary_error: 0.227929	valid_1's binary_error: 0.309701
    [95]	training's binary_error: 0.227929	valid_1's binary_error: 0.309701
    [96]	training's binary_error: 0.227929	valid_1's binary_error: 0.309701
    [97]	training's binary_error: 0.227929	valid_1's binary_error: 0.309701
    [98]	training's binary_error: 0.227929	valid_1's binary_error: 0.309701
    [99]	training's binary_error: 0.221509	valid_1's binary_error: 0.317164
    [100]	training's binary_error: 0.227929	valid_1's binary_error: 0.309701
    [101]	training's binary_error: 0.23114	valid_1's binary_error: 0.30597
    [102]	training's binary_error: 0.23114	valid_1's binary_error: 0.30597
    [103]	training's binary_error: 0.227929	valid_1's binary_error: 0.309701
    [104]	training's binary_error: 0.221509	valid_1's binary_error: 0.317164
    [105]	training's binary_error: 0.221509	valid_1's binary_error: 0.317164
    [106]	training's binary_error: 0.224719	valid_1's binary_error: 0.313433
    [107]	training's binary_error: 0.224719	valid_1's binary_error: 0.317164
    [108]	training's binary_error: 0.224719	valid_1's binary_error: 0.317164
    [109]	training's binary_error: 0.224719	valid_1's binary_error: 0.317164
    [110]	training's binary_error: 0.224719	valid_1's binary_error: 0.317164
    [111]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [112]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [113]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [114]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [115]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [116]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [117]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [118]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [119]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [120]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [121]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [122]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [123]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [124]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [125]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [126]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [127]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [128]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [129]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [130]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [131]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [132]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [133]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [134]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [135]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [136]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [137]	training's binary_error: 0.219904	valid_1's binary_error: 0.309701
    [138]	training's binary_error: 0.219904	valid_1's binary_error: 0.309701
    [139]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [140]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [141]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [142]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [143]	training's binary_error: 0.223114	valid_1's binary_error: 0.309701
    [144]	training's binary_error: 0.221509	valid_1's binary_error: 0.320896
    [145]	training's binary_error: 0.223114	valid_1's binary_error: 0.313433
    [146]	training's binary_error: 0.221509	valid_1's binary_error: 0.313433
    [147]	training's binary_error: 0.221509	valid_1's binary_error: 0.313433
    [148]	training's binary_error: 0.221509	valid_1's binary_error: 0.313433
    [149]	training's binary_error: 0.221509	valid_1's binary_error: 0.313433
    [150]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [151]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [152]	training's binary_error: 0.221509	valid_1's binary_error: 0.313433
    [153]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [154]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [155]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [156]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [157]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [158]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [159]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [160]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [161]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [162]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [163]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [164]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [165]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [166]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [167]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [168]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [169]	training's binary_error: 0.219904	valid_1's binary_error: 0.324627
    [170]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [171]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [172]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [173]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [174]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [175]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [176]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [177]	training's binary_error: 0.221509	valid_1's binary_error: 0.328358
    [178]	training's binary_error: 0.221509	valid_1's binary_error: 0.328358
    [179]	training's binary_error: 0.221509	valid_1's binary_error: 0.328358
    [180]	training's binary_error: 0.221509	valid_1's binary_error: 0.328358
    [181]	training's binary_error: 0.221509	valid_1's binary_error: 0.328358
    [182]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [183]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [184]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [185]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [186]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [187]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [188]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [189]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [190]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [191]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [192]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [193]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [194]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [195]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [196]	training's binary_error: 0.216693	valid_1's binary_error: 0.320896
    [197]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [198]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [199]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [200]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    [201]	training's binary_error: 0.215088	valid_1's binary_error: 0.317164
    Early stopping, best iteration is:
    [101]	training's binary_error: 0.23114	valid_1's binary_error: 0.30597
    

    /usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument
      warnings.warn("Found `{}` in params. Will use it instead of argument".format(alias))
    




    0.6940298507462687


