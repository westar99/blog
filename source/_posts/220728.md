---

# **title: 'Airflow & Pyspark'**

# **date: '2022-07-28 09:00'**

---


    
- 에어플로우 튜토리얼 링크

[Tutorial - Airflow Documentation](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html)

- 데이터 파이프라인

[Apache Airflow 기반의 데이터 파이프라인 - YES24](http://www.yes24.com/Product/Goods/107878326)

```
import datetime as dt
from datetime import timedelta

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

import pandas as pd

def csvToJson():
    df=pd.read_csv('data.csv')
    for i,r in df.iterrows():
        print(r['name'])
    df.to_json('fromAirflow.json',orient='records')

default_args = {
    'owner': 'evan',
    'start_date': dt.datetime(2022, 7, 27),
    'retries': 1,
    'retry_delay': dt.timedelta(minutes=1),
}

with DAG('MyCSVDAG',
         default_args=default_args,
         schedule_interval=timedelta(minutes=1),      # '0 * * * *',
         ) as dag:

    print_starting = BashOperator(task_id='starting',
                               bash_command='echo "I am reading the CSV now....."')

    csvJson = PythonOperator(task_id='convertCSVtoJson',
                                 python_callable=csvToJson)

print_starting >> csvJson
```

—>중간에 data.csv를 dags/data.csv로 위치 변경해줌

- 가상환경에서 에어플로우 실행
- 리눅스(우분투)실행
    
    ![](images/220728/Untitled2.png)
    
- vs코드 실행되면서  아래와 같이 쳐줌
- source venv/bin/activate (가상환경설정)
- airflow db reset (데이터베이스 리셋)
- airflow webserver -p 8081 (로컬호스트 명 결정)
    
    ![](images/220728/Untitled3.png)
    
    ![](images/220728/Untitled4.png)
    
- 다른 터미널을 열고 airflow scheduler 실행
- 웹사이트에 localhost:8081 실행
- 아이디와 암호를 넣으면 에어플로어가 돌아간다

## 아파치 스파크

- 폴더를 만들고 다음과 같이 친다.
    
    ![](images/220728/Untitled5.png)
    
- `sudo apt-get install openjdk-8-jdk`
- `sudo wget [https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz](https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz)` (압축파일을 다운로드 받음)
- mv 는 폴더를 통체로 옮기는 명령어이다.
    
    ![](images/220728/Untitled6.png)
    
    ![](images/220728/Untitled7.png)
    
    ![](images/220728/Untitled8.png)
    
    ![](images/220728/Untitled9.png)
    

- 직렬 방식은 처리 시간이 느리지만  구글의 병렬방식(hadoop)로  처리하면 빠르다. 이를 더 업그래이드 시킨 것이 spark이다. (분산처리 방식)

![](images/220728/Untitled10.png)

mv이동 cp복사

- cp -r spark3/ spark-node # spar3를 spark-node로 복사하라.
    
    ![](images/220728/Untitled11.png)
    
    ![](images/220728/Untitled12.png)
    
- 리눅스(우분투)로 들어가서 환경변수를 설정해야 한다.
- vi ~/.bashrc  를 넣으면 환경설정에 들어간다.(i 눌러야 키가 들어가고 끝나면 esc)
- 환경변수편집과 같은 것.
    
    ![](images/220728/Untitled13.png)
    

export AIRFLOW_HOME=/home/human/airflow
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export SPARK_HOME=/home/human/spark3
export PATH=$SPARK_HOME/bin:$PATH
export PYSPARK_PYTHON=python3
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSAPRK_DRIVER_PYTHON_OPTS='notebook'

- :wq!를 쳐주면 터미널로 나간다.(wright quit!)
- source ~/.bashrc 하면 저장이 됨
- 리스타트를 하고 pyspark를 써주면 이런 화면이 나온다.
    
    ![](images/220728/Untitled14.png)
    
    ![](images/220728/Untitled15.png)
    
    ![](images/220728/Untitled16.png)
    
    → 가상환경을 만들어 준다.
    
    - pip3 install pyspark
    - pip3 install findspark
    - pip3 install jupyter 를 차례대로 실행하여 깔아준다.
    
    → 새 터미널 bash를 띄워서 편집을 해야 한다. 안그러면 가상환경이 풀린다.
    
    ![](images/220728/Untitled17.png)
    
    vi ~/.bashrc (
    
    source ~/.bashrc
    
    jupyter notebook
    
    ![](images/220728/Untitled18.png)
    
    ![](images/220728/Untitled19.png)
    
    ![](images/220728/Untitled20.png)